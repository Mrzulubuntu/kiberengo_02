    async def connect(self, symbol: str, interval: str) -> bool:
        """
        🔌 Connect to Binance WebSocket stream
        
        Args:
            symbol: Trading symbol (e.g., 'BTCUSDT')
            interval: Time interval (e.g., '1m', '1h', '1d')
            
        Returns:
            True if connection successful, False otherwise
        """
        print(f"🔌 Connecting to {symbol} {interval} stream...")
        
        # 🔄 Clean shutdown of existing connection
        await self.disconnect()
        
        # 📊 Update configuration
        self.current_symbol = symbol
        self.current_interval = interval
        self.stream_url = self._build_stream_url(symbol, interval)
        
        try:
            # 🚀 Establish WebSocket connection
            self.websocket = await websockets.connect(
                self.stream_url,
                ping_interval=20,
                ping_timeout=10,
                close_timeout=10
            )
            
            self.is_connected = True
            self.reconnect_attempts = 0
            
            print(f"✅ Connected to {symbol} {interval} stream")
            logger.info(f"WebSocket connected: {self.stream_url}")
            
            # 🔥 Start listening for data
            asyncio.create_task(self._listen_for_data())
            
            return True
            
        except Exception as e:
            error_msg = f"❌ Connection failed: {e}"
            print(error_msg)
            logger.error(error_msg)
            self.error_handler.handle_connection_error(e)
            return False

    async def disconnect(self):
        """
        🔐 Clean disconnect from WebSocket
        Ensures proper cleanup of resources
        """
        if self.websocket and not self.websocket.closed:
            self.is_connected = False
            await self.websocket.close()
            print(f"🔐 Disconnected from {self.current_symbol} {self.current_interval}")
            logger.info("WebSocket connection closed cleanly")

    async def _listen_for_data(self):
        """
        👂 Listen for incoming WebSocket data
        Processes kline data and triggers callbacks
        """
        print(f"👂 Listening for {self.current_symbol} {self.current_interval} data...")
        
        try:
            async for message in self.websocket:
                if not self.is_connected:
                    break
                    
                # 📊 Process incoming kline data
                await self._process_kline_data(message)
                
        except websockets.exceptions.ConnectionClosed:
            print("🔌 WebSocket connection closed")
            if self.should_reconnect:
                await self._attempt_reconnection()
                
        except Exception as e:
            error_msg = f"❌ Data listening error: {e}"
            print(error_msg)
            logger.error(error_msg)
            self.error_handler.handle_data_error(e)
            
            if self.should_reconnect:
                await self._attempt_reconnection()

    async def _process_kline_data(self, message: str):
        """
        🔧 Process incoming kline (candlestick) data
        Converts to OHLCData and triggers storage + callbacks
        """
        try:
            data = json.loads(message)
            
            # 📊 Extract kline data from Binance format
            kline = data.get('k', {})
            if not kline:
                return
                
            # 🕐 Convert timestamp to datetime
            timestamp = datetime.fromtimestamp(kline['T'] / 1000)
            
            # 💰 Create OHLC data object
            ohlc_data = OHLCData(
                timestamp=timestamp,
                symbol=kline['s'],
                interval=kline['i'],
                open=float(kline['o']),
                high=float(kline['h']),
                low=float(kline['l']),
                close=float(kline['c']),
                volume=float(kline['v'])
            )
            
            # 📈 Calculate indicators for real-time data
            # Note: For real-time indicators, we need recent historical data
            historical_df = self.data_manager.get_historical_data(
                self.current_symbol, 
                self.current_interval, 
                limit=100  # Last 100 candles for indicator calculations
            )
            
            if len(historical_df) > 0:
                # 🔧 Add current candle to historical data
                current_df = pl.DataFrame([{
                    'timestamp': ohlc_data.timestamp,
                    'symbol': ohlc_data.symbol,
                    'interval': ohlc_data.interval,
                    'open': ohlc_data.open,
                    'high': ohlc_data.high,
                    'low': ohlc_data.low,
                    'close': ohlc_data.close,
                    'volume': ohlc_data.volume
                }])
                
                # 📊 Combine historical + current data
                combined_df = pl.concat([historical_df, current_df])
                
                # 🎯 Calculate indicators
                with_indicators = self.indicator_manager.calculate_all_indicators(combined_df)
                
                # 📈 Extract indicator values for current candle
                if len(with_indicators) > 0:
                    latest_row = with_indicators.row(-1, named=True)
                    
                    # 🔄 Update OHLC data with indicators
                    ohlc_data.ema_12 = latest_row.get('ema_12')
                    ohlc_data.ema_26 = latest_row.get('ema_26')
                    ohlc_data.rsi = latest_row.get('rsi')
                    ohlc_data.macd = latest_row.get('macd')
                    ohlc_data.macd_signal = latest_row.get('macd_signal')
                    ohlc_data.macd_histogram = latest_row.get('macd_histogram')
                    ohlc_data.bb_upper = latest_row.get('bb_upper')
                    ohlc_data.bb_middle = latest_row.get('bb_middle')
                    ohlc_data.bb_lower = latest_row.get('bb_lower')
                    ohlc_data.stoch_k = latest_row.get('stoch_k')
                    ohlc_data.stoch_d = latest_row.get('stoch_d')
            
            # 💾 Store in database
            self.data_manager.store_ohlc_batch([ohlc_data])
            
            # 📡 Trigger callbacks for real-time updates
            for callback in self.data_callbacks:
                try:
                    await callback(self.current_symbol, self.current_interval, ohlc_data)
                except Exception as e:
                    logger.error(f"Callback error: {e}")
            
            # 📊 Log progress
            print(f"📊 {ohlc_data.symbol} {ohlc_data.interval} | "
                  f"Close: ${ohlc_data.close:.2f} | "
                  f"RSI: {ohlc_data.rsi:.1f if ohlc_data.rsi else 'N/A'}")
            
        except Exception as e:
            error_msg = f"❌ Data processing error: {e}"
            print(error_msg)
            logger.error(error_msg)
            self.error_handler.handle_processing_error(e)

    async def _attempt_reconnection(self):
        """
        🔄 Attempt to reconnect after connection loss
        Implements exponential backoff for reliability
        """
        if self.reconnect_attempts >= self.max_reconnect_attempts:
            print(f"❌ Max reconnection attempts reached ({self.max_reconnect_attempts})")
            self.should_reconnect = False
            return
            
        self.reconnect_attempts += 1
        delay = self.reconnect_delay * (2 ** (self.reconnect_attempts - 1))  # Exponential backoff
        
        print(f"🔄 Reconnection attempt {self.reconnect_attempts}/{self.max_reconnect_attempts} in {delay}s...")
        await asyncio.sleep(delay)
        
        success = await self.connect(self.current_symbol, self.current_interval)
        if not success:
            await self._attempt_reconnection()

    async def switch_symbol_interval(self, symbol: str, interval: str):
        """
        🔄 Dynamically switch to different symbol/interval
        Cleanly tears down current connection and establishes new one
        """
        print(f"🔄 Switching from {self.current_symbol}_{self.current_interval} to {symbol}_{interval}")
        
        # 🔐 Clean shutdown of current stream
        await self.disconnect()
        
        # 🔌 Connect to new stream
        success = await self.connect(symbol, interval)
        
        if success:
            print(f"✅ Successfully switched to {symbol} {interval}")
        else:
            print(f"❌ Failed to switch to {symbol} {interval}")
            
        return success

    def stop(self):
        """
        🛑 Stop WebSocket manager
        Prevents reconnection attempts
        """
        self.should_reconnect = False
        print("🛑 WebSocket manager stopped")
        logger.info("WebSocket manager shutdown initiated")
```

---

## 🏗️ Phase 5: Utility Classes (Logging, Progress Bars, Error Handling)

### 5.1 Custom Logger Implementation

```python
# apps/trading_platform/utils/logger.py
"""
📝 Advanced Logging System
Provides comprehensive logging with multiple output formats
"""
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional
import json

class ColoredFormatter(logging.Formatter):
    """
    🎨 Colored console formatter for beautiful terminal output
    """
    
    # 🌈 Color codes for different log levels
    COLORS = {
        'DEBUG': '\033[36m',     # Cyan
        'INFO': '\033[32m',      # Green
        'WARNING': '\033[33m',   # Yellow
        'ERROR': '\033[31m',     # Red
        'CRITICAL': '\033[35m',  # Magenta
        'RESET': '\033[0m'       # Reset
    }
    
    def format(self, record):
        """Format log record with colors and emojis"""
        # 📅 Add timestamp
        record.timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # 🎨 Add colors
        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.colored_levelname = f"{color}{record.levelname}{self.COLORS['RESET']}"
        
        # 🎯 Add emoji based on level
        emoji_map = {
            'DEBUG': '🔍',
            'INFO': '📊',
            'WARNING': '⚠️',
            'ERROR': '❌',
            'CRITICAL': '🚨'
        }
        record.emoji = emoji_map.get(record.levelname, '📝')
        
        # 📝 Format message
        format_str = f"{record.emoji} [{record.timestamp}] {record.colored_levelname} - {record.getMessage()}"
        return format_str

class TradingPlatformLogger:
    """
    📊 Comprehensive logging system for trading platform
    Features multiple handlers and structured logging
    """
    
    def __init__(self, name: str, log_dir: str = "logs"):
        """
        Initialize logger with multiple handlers
        
        Args:
            name: Logger name (usually __name__)
            log_dir: Directory for log files
        """
        self.name = name
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        # 📊 Create logger
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.DEBUG)
        
        # 🔄 Clear existing handlers
        self.logger.handlers.clear()
        
        # 🖥️ Console handler with colors
        self._setup_console_handler()
        
        # 📁 File handlers
        self._setup_file_handlers()
        
        # 📊 Performance metrics
        self.start_time = datetime.now()

    def _setup_console_handler(self):
        """Setup colorized console output"""
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(ColoredFormatter())
        self.logger.addHandler(console_handler)

    def _setup_file_handlers(self):
        """Setup file handlers for different log levels"""
        # 📝 General log file
        general_handler = logging.FileHandler(
            self.log_dir / f"trading_platform_{datetime.now().strftime('%Y%m%d')}.log"
        )
        general_handler.setLevel(logging.DEBUG)
        general_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        general_handler.setFormatter(general_formatter)
        self.logger.addHandler(general_handler)
        
        # ❌ Error log file
        error_handler = logging.FileHandler(
            self.log_dir / f"errors_{datetime.now().strftime('%Y%m%d')}.log"
        )
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(general_formatter)
        self.logger.addHandler(error_handler)
        
        # 📊 Performance log file (JSON format)
        self.perf_log_path = self.log_dir / f"performance_{datetime.now().strftime('%Y%m%d')}.jsonl"

    def log_performance(self, operation: str, duration: float, **kwargs):
        """
        📊 Log performance metrics in JSON format
        Perfect for analysis and monitoring
        """
        perf_data = {
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'duration_ms': duration * 1000,
            'module': self.name,
            **kwargs
        }
        
        with open(self.perf_log_path, 'a') as f:
            f.write(json.dumps(perf_data) + '\n')

    def log_trade_data(self, symbol: str, interval: str, ohlc_data: dict):
        """
        📈 Log trading data for audit trail
        """
        trade_log = {
            'timestamp': datetime.now().isoformat(),
            'symbol': symbol,
            'interval': interval,
            'ohlc': ohlc_data,
            'module': self.name
        }
        
        trade_log_path = self.log_dir / f"trades_{datetime.now().strftime('%Y%m%d')}.jsonl"
        with open(trade_log_path, 'a') as f:
            f.write(json.dumps(trade_log) + '\n')

def get_logger(name: str) -> logging.Logger:
    """
    🎯 Factory function to get configured logger
    
    Usage:
        logger = get_logger(__name__)
        logger.info("This is a test message")
    """
    trading_logger = TradingPlatformLogger(name)
    return trading_logger.logger

### 5.2 Custom Progress Bar Implementation

```python
# apps/trading_platform/utils/progress_bar.py
"""
📊 Beautiful Custom Progress Bars
Enhanced tqdm with colors, animations, and custom styles
"""
import time
import random
from typing import Optional, List
from tqdm import tqdm
import threading

class CustomProgressBar:
    """
    🎨 Enhanced Progress Bar with Multiple Styles
    
    Features:
    - Color transitions every 20%
    - Multiple bar styles (blocks, stars, pipes)
    - Inline progress display
    - Custom animations
    - Performance metrics
    """
    
    # 🎨 Color codes for different progress stages
    COLORS = [
        '\033[31m',  # Red (0-20%)
        '\033[33m',  # Yellow (20-40%)
        '\033[36m',  # Cyan (40-60%)
        '\033[34m',  # Blue (60-80%)
        '\033[32m',  # Green (80-100%)
    ]
    RESET = '\033[0m'
    
    # 📊 Different bar styles
    BAR_STYLES = [
        {'filled': '█', 'empty': '░'},      # Blocks
        {'filled': '★', 'empty': '☆'},      # Stars  
        {'filled': '|', 'empty': '-'},      # Pipes
        {'filled': '●', 'empty': '○'},      # Circles
        {'filled': '♦', 'empty': '♢'},      # Diamonds
    ]
    
    def __init__(self, total: int, desc: str = "Processing", width: int = 50):
        """
        Initialize custom progress bar
        
        Args:
            total: Total number of items to process
            desc: Description text
            width: Width of progress bar
        """
        self.total = total
        self.desc = desc
        self.width = width
        self.current = 0
        self.start_time = time.time()
        
        # 🎲 Randomly select bar style
        self.style = random.choice(self.BAR_STYLES)
        
        # 🎯 Initialize tqdm with custom settings
        self.pbar = tqdm(
            total=total,
            desc=desc,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',
            ncols=100,
            position=0,
            leave=True
        )
        
        print(f"🚀 Progress tracking started: {desc}")

    def update(self, n: int = 1):
        """
        📈 Update progress bar
        
        Args:
            n: Number of items processed
        """
        self.current += n
        self.pbar.update(n)
        
        # 🎨 Update color based on progress percentage
        progress_percent = (self.current / self.total) * 100
        color_index = min(int(progress_percent // 20), len(self.COLORS) - 1)
        
        # 📊 Custom inline display
        if self.current % max(1, self.total // 20) == 0:  # Update every 5%
            self._display_custom_progress(progress_percent, color_index)

    def _display_custom_progress(self, percent: float, color_index: int):
        """
        🎨 Display custom progress visualization
        """
        filled_length = int(self.width * percent / 100)
        
        # 🎨 Create colored bar
        color = self.COLORS[color_index]
        filled_bar = color + (self.style['filled'] * filled_length) + self.RESET
        empty_bar = self.style['empty'] * (self.width - filled_length)
        
        # ⏱️ Calculate speed
        elapsed = time.time() - self.start_time
        speed = self.current / elapsed if elapsed > 0 else 0
        
        # 📊 Display custom progress
        custom_display = f"[{filled_bar}{empty_bar}] {percent:.1f}% | Speed: {speed:.1f} items/s"
        print(f"\r{custom_display}", end='', flush=True)

    def close(self):
        """
        ✅ Close progress bar and show completion stats
        """
        self.pbar.close()
        
        # 📊 Calculate final stats
        elapsed = time.time() - self.start_time
        avg_speed = self.total / elapsed if elapsed > 0 else 0
        
        print(f"\n✅ {self.desc} completed!")
        print(f"📊 Total items: {self.total}")
        print(f"⏱️ Time elapsed: {elapsed:.2f}s")
        print(f"🚀 Average speed: {avg_speed:.2f} items/s")

class MultiProgressTracker:
    """
    📊 Track multiple progress bars simultaneously
    Perfect for parallel operations
    """
    
    def __init__(self):
        """Initialize multi-progress tracker"""
        self.progress_bars = {}
        self.lock = threading.Lock()
        
    def add_progress(self, name: str, total: int, desc: str = None) -> CustomProgressBar:
        """
        ➕ Add new progress bar
        
        Args:
            name: Unique identifier for this progress
            total: Total items to process
            desc: Description text
            
        Returns:
            CustomProgressBar instance
        """
        with self.lock:
            desc = desc or f"Processing {name}"
            pbar = CustomProgressBar(total, desc)
            self.progress_bars[name] = pbar
            return pbar
    
    def update_progress(self, name: str, n: int = 1):
        """
        📈 Update specific progress bar
        
        Args:
            name: Progress bar identifier
            n: Number of items processed
        """
        if name in self.progress_bars:
            self.progress_bars[name].update(n)
    
    def close_progress(self, name: str):
        """
        ✅ Close specific progress bar
        
        Args:
            name: Progress bar identifier
        """
        if name in self.progress_bars:
            self.progress_bars[name].close()
            del self.progress_bars[name]
    
    def close_all(self):
        """
        🔒 Close all progress bars
        """
        with self.lock:
            for pbar in self.progress_bars.values():
                pbar.close()
            self.progress_bars.clear()
            print("📊 All progress tracking completed!")

### 5.3 Comprehensive Error Handler

```python
# apps/trading_platform/utils/error_handler.py
"""
🛡️ Comprehensive Error Handling System
Handles various types of errors with appropriate recovery strategies
"""
import time
import traceback
from datetime import datetime
from typing import Dict, Optional, Callable, Any
from enum import Enum
import asyncio

from .logger import get_logger

logger = get_logger(__name__)

class ErrorType(Enum):
    """
    🏷️ Error classification for appropriate handling
    """
    CONNECTION_ERROR = "connection"
    API_ERROR = "api"
    DATA_ERROR = "data"
    PROCESSING_ERROR = "processing"
    VALIDATION_ERROR = "validation"
    SYSTEM_ERROR = "system"

class ErrorSeverity(Enum):
    """
    ⚠️ Error severity levels
    """
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class ErrorHandler:
    """
    🛡️ Advanced Error Handling System
    
    Features:
    - Automatic retry mechanisms
    - Error classification and severity assessment
    - Recovery strategies
    - Comprehensive logging
    - Rate limiting for error reporting
    """
    
    def __init__(self):
        """Initialize error handler with default settings"""
        
        # 🔄 Retry configuration
        self.retry_config = {
            ErrorType.CONNECTION_ERROR: {'max_retries': 5, 'base_delay': 2.0, 'backoff': 2.0},
            ErrorType.API_ERROR: {'max_retries': 3, 'base_delay': 1.0, 'backoff': 1.5},
            ErrorType.DATA_ERROR: {'max_retries': 2, 'base_delay': 0.5, 'backoff': 1.0},
            ErrorType.PROCESSING_ERROR: {'max_retries': 1, 'base_delay': 0.1, 'backoff': 1.0},
        }
        
        # 📊 Error tracking
        self.error_counts: Dict[ErrorType, int] = {etype: 0 for etype in ErrorType}
        self.last_error_time: Dict[ErrorType, float] = {}
        
        # 🔔 Notification settings (commented out initially)
        self.notification_enabled = False
        self.notification_threshold = {
            ErrorSeverity.LOW: 10,
            ErrorSeverity.MEDIUM: 5,
            ErrorSeverity.HIGH: 2,
            ErrorSeverity.CRITICAL: 1
        }
        
        print("🛡️ Error Handler initialized with comprehensive recovery strategies")
        logger.info("ErrorHandler ready for error management")

    def handle_error(
        self, 
        error: Exception, 
        error_type: ErrorType, 
        context: str = "",
        severity: ErrorSeverity = ErrorSeverity.MEDIUM,
        retry_func: Optional[Callable] = None,
        **kwargs
    ) -> bool:
        """
        🔧 Main error handling function
        
        Args:
            error: The exception that occurred
            error_type: Classification of the error
            context: Additional context about where the error occurred
            severity: How severe this error is
            retry_func: Function to retry if applicable
            **kwargs: Additional parameters for retry function
            
        Returns:
            True if error was handled successfully, False otherwise
        """
        # 📊 Update error tracking
        self.error_counts[error_type] += 1
        self.last_error_time[error_type] = time.time()
        
        # 📝 Log error details
        error_msg = f"🚨 {error_type.value.upper()} ERROR in {context}: {str(error)}"
        
        if severity == ErrorSeverity.CRITICAL:
            logger.critical(error_msg)
            print(f"🚨 CRITICAL ERROR: {error}")
        elif severity == ErrorSeverity.HIGH:
            logger.error(error_msg)
            print(f"❌ HIGH SEVERITY: {error}")
        elif severity == ErrorSeverity.MEDIUM:
            logger.warning(error_msg)
            print(f"⚠️ MEDIUM SEVERITY: {error}")
        else:
            logger.info(error_msg)
            print(f"ℹ️ LOW SEVERITY: {error}")
        
        # 📊 Log stack trace for debugging
        logger.debug(f"Stack trace: {traceback.format_exc()}")
        
        # 🔄 Attempt recovery/retry if applicable
        if retry_func and error_type in self.retry_config:
            return self._attempt_retry(error_type, retry_func, **kwargs)
        
        # 🔔 Check if notification should be sent
        self._check_notification_threshold(error_type, severity)
        
        return False

    def _attempt_retry(self, error_type: ErrorType, retry_func: Callable, **kwargs) -> bool:
        """
        🔄 Attempt to retry failed operation with exponential backoff
        
        Args:
            error_type: Type of error for retry configuration
            retry_func: Function to retry
            **kwargs: Parameters for retry function
            
        Returns:
            True if retry succeeded, False if all retries failed
        """
        config = self.retry_config[error_type]
        max_retries = config['max_retries']
        base_delay = config['base_delay']
        backoff = config['backoff']
        
        for attempt in range(max_retries):
            try:
                # ⏱️ Calculate delay with exponential backoff
                delay = base_delay * (backoff ** attempt)
                
                print(f"🔄 Retry attempt {attempt + 1}/{max_retries} in {delay:.1f}s...")
                time.sleep(delay)
                
                # 🚀 Attempt retry
                result = retry_func(**kwargs)
                
                print(f"✅ Retry successful after {attempt + 1} attempts")
                logger.info(f"Recovery successful: {error_type.value} after {attempt + 1} attempts")
                
                return True
                
            except Exception as e:
                print(f"❌ Retry {attempt + 1} failed: {e}")
                logger.warning(f"Retry {attempt + 1} failed: {e}")
                
                if attempt == max_retries - 1:
                    print(f"🚫 All {max_retries} retry attempts failed")
                    logger.error(f"All retry attempts failed for {error_type.value}")
        
        return False

    async def async_retry(self, error_type: ErrorType, retry_func: Callable, **kwargs) -> bool:
        """
        🔄 Async version of retry mechanism
        """
        config = self.retry_config[error_type]
        max_retries = config['max_retries']
        base_delay = config['base_delay']
        backoff = config['backoff']
        
        for attempt in range(max_retries):
            try:
                delay = base_delay * (backoff ** attempt)
                
                print(f"🔄 Async retry {attempt + 1}/{max_retries} in {delay:.1f}s...")
                await asyncio.sleep(delay)
                
                result = await retry_func(**kwargs)
                
                print(f"✅ Async retry successful after {attempt + 1} attempts")
                return True
                
            except Exception as e:
                print(f"❌ Async retry {attempt + 1} failed: {e}")
                
                if attempt == max_retries - 1:
                    print(f"🚫 All async retry attempts failed")
        
        return False

    def _check_notification_threshold(self, error_type: ErrorType, severity: ErrorSeverity):
        """
        🔔 Check if error threshold reached for notifications
        """
        if not self.notification_enabled:
            return
            
        threshold = self.notification_threshold[severity]
        count = self.error_counts[error_type]
        
        if count >= threshold:
            self._send_notification(error_type, severity, count)

    def _send_notification(self, error_type: ErrorType, severity: ErrorSeverity, count: int):
        """
        📧 Send notification about error threshold
        (Implementation commented out until email/telegram setup)
        """
        message = f"🚨 Error threshold reached: {error_type.value} ({severity.value}) - {count} occurrences"
        
        print(f"🔔 NOTIFICATION: {message}")
        logger.warning(f"Notification triggered: {message}")
        
        # TODO: Implement when email/telegram is set up
        # self._send_email_notification(message)
        # self._send_telegram_notification